{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mamta691/review_scrap_aws/blob/main/Copy_of_sparknlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lje59LpEXpgF",
        "outputId": "3ad9cbbd-685a-4fac-b524-4f3097cf01e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=18c74267735cf1c18e342f9f5b4c0739a4ebc47021bb791010bbdd180fd323b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3VO5TMTXpgH",
        "outputId": "f07fe948-055a-4f92-94d4-a838c8e590de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spark-nlp\n",
            "  Downloading spark_nlp-5.1.3-py2.py3-none-any.whl (537 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/537.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/537.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m532.5/537.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m537.5/537.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: spark-nlp\n",
            "Successfully installed spark-nlp-5.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install spark-nlp\n",
        "# !pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPlCeBhdXpgI",
        "outputId": "cd164b86-8d46-4a1b-bc6e-93230de41861",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJ5sgF5RXpgI"
      },
      "outputs": [],
      "source": [
        "import sparknlp\n",
        "\n",
        "# Start Spark Session\n",
        "spark = sparknlp.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvE9lZidXpgJ"
      },
      "outputs": [],
      "source": [
        "# Create a dataframe from the sample_text\n",
        "data = spark.createDataFrame([\n",
        "[\"\"\"As she traveled across the world, Emma visited many different places\n",
        "and met many fascinating people. She walked the busy streets of Tokyo,\n",
        "hiked the rugged mountains of Nepal, and swam in the crystal-clear waters\n",
        "of the Caribbean. Along the way, she befriended locals like Akira, Rajesh,\n",
        "and Maria, each with their own unique stories to tell. Emma's travels took her\n",
        "to many cities, including New York, Paris, and Hong Kong, where she savored\n",
        "delicious foods and explored vibrant cultures. No matter where she went,\n",
        "Emma always found new wonders to discover and memories to cherish.\"\"\"]\n",
        "]).toDF(\"text\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wbBAtDHCenXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20_Jjlj0XpgJ"
      },
      "outputs": [],
      "source": [
        "# PERSON\n",
        "person_matches = \"\"\"\n",
        "Emma\n",
        "Akira\n",
        "Rajesh\n",
        "Maria\n",
        "\"\"\"\n",
        "\n",
        "with open('person_matches.txt', 'w') as f:\n",
        "    f.write(person_matches)\n",
        "\n",
        "# LOCATION\n",
        "location_matches = \"\"\"\n",
        "Tokyo\n",
        "Nepal\n",
        "Caribbean\n",
        "New York\n",
        "Paris\n",
        "Hong Kong\n",
        "\"\"\"\n",
        "\n",
        "with open('location_matches.txt', 'w') as f:\n",
        "    f.write(location_matches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4m9YH_R-XpgK"
      },
      "outputs": [],
      "source": [
        "# Import the required modules and classes\n",
        "from sparknlp.base import DocumentAssembler, Pipeline, ReadAs\n",
        "from sparknlp.annotator import (\n",
        "    Tokenizer,\n",
        "    TextMatcher\n",
        ")\n",
        "from pyspark.sql.types import StringType\n",
        "import pyspark.sql.functions as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKJxv1PTXpgL",
        "outputId": "83ec38ab-b0a4-4e3b-a09d-e38c91d7ac8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bb2c49c97f19>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Step 1: Transforms raw texts to `document` annotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdocument_assembler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDocumentAssembler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0msetInputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0msetOutputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"document\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DocumentAssembler' is not defined"
          ]
        }
      ],
      "source": [
        "# Step 1: Transforms raw texts to `document` annotation\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"text\") \\\n",
        "    .setOutputCol(\"document\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bI5U6LWYXpgL"
      },
      "outputs": [],
      "source": [
        "# Step 2: Gets the tokens of the text\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols(\"document\") \\\n",
        "    .setOutputCol(\"token\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27JoCwUJXpgL"
      },
      "outputs": [],
      "source": [
        "# Step 3: PERSON matcher\n",
        "person_extractor = TextMatcher() \\\n",
        "    .setInputCols(\"document\", \"token\") \\\n",
        "    .setEntities(\"person_matches.txt\", ReadAs.TEXT) \\\n",
        "    .setEntityValue(\"PERSON\") \\\n",
        "    .setOutputCol(\"person_entity\") \\\n",
        "    .setCaseSensitive(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gpKuEklXpgM"
      },
      "outputs": [],
      "source": [
        "# Step 4: LOCATION matcher\n",
        "location_extractor = TextMatcher() \\\n",
        "    .setInputCols(\"document\", \"token\") \\\n",
        "    .setEntities(\"location_matches.txt\", ReadAs.TEXT) \\\n",
        "    .setEntityValue(\"LOCATION\") \\\n",
        "    .setOutputCol(\"location_entity\") \\\n",
        "    .setCaseSensitive(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGVbiNqKXpgN"
      },
      "outputs": [],
      "source": [
        "pipeline = Pipeline().setStages([document_assembler,\n",
        "                                 tokenizer,\n",
        "                                 person_extractor,\n",
        "                                 location_extractor\n",
        "                                 ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JU9e9NR2XpgO",
        "outputId": "7390ed13-495b-4a5e-8361-f34dd0b7b994"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------------+\n",
            "|result                            |\n",
            "+----------------------------------+\n",
            "|[Emma, Akira, Rajesh, Maria, Emma]|\n",
            "+----------------------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Fit and transform to get a prediction\n",
        "results = pipeline.fit(data).transform(data)\n",
        "\n",
        "# Display the results\n",
        "results.selectExpr(\"person_entity.result\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9M8F1McsXpgP",
        "outputId": "8290ddbb-5e40-44d7-e6c4-b0cefd0182f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------+\n",
            "|result                                               |\n",
            "+-----------------------------------------------------+\n",
            "|[Tokyo, Nepal, Caribbean, New York, Paris, Hong Kong]|\n",
            "+-----------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "results.selectExpr(\"location_entity.result\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWeY6g-1XpgP"
      },
      "source": [
        "In this example, we created two TextMatcher stages, one matches person names and the other stage matches locations. Once the Spark NLP pipeline is applied to the sample text, any words that match the specified words are extracted.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}